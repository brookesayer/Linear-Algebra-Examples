{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hands3d(ax, points, color, linewidth='3'):\n",
    "    # Add bone connections\n",
    "    bones = [(13, 11),\n",
    "             (13, 12),\n",
    "             (13, 1),\n",
    "             (0, 1),\n",
    "             (13, 3),\n",
    "             (2, 3),\n",
    "             (13, 5),\n",
    "             (4, 5),\n",
    "             (13, 7),\n",
    "             (6, 7),\n",
    "             (13, 10),\n",
    "             (9, 10),\n",
    "             (8, 9)]\n",
    "\n",
    "    for connection in bones:\n",
    "        coord1 = points[connection[0]]\n",
    "        coord2 = points[connection[1]]\n",
    "        coords = np.stack([coord1, coord2])\n",
    "        ax.plot(coords[:, 0], coords[:, 1], coords[:, 2], c=color, linewidth=linewidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYU Hand Pose Dataset\n",
    "\n",
    "The [NYU Hand Pose Dataset](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm) was published in 2014. It provides 72,757 training frames and 8,252 test frames captured with RGB-D cameras (Kinect 1). Keypoint annotations are provided for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "testing_mat = sio.loadmat(\"/Users/sayer/Downloads/test_joint_data.mat\")\n",
    "training_mat = sio.loadmat(\"/Users/sayer/Downloads/train_joint_data.mat\")\n",
    "test_dataset = testing_mat['joint_uvd'][0]\n",
    "train_dataset = training_mat['joint_uvd'][0]\n",
    "\n",
    "# Show a sample\n",
    "eval_joints = [0, 3, 6, 9, 12, 15, 18, 21, 24, 25, 27, 30, 31, 32]\n",
    "train_dataset = train_dataset[:, eval_joints]\n",
    "test_dataset = test_dataset[:, eval_joints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[10000]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(-90, -90)\n",
    "plot_hands3d(ax, sample, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "Normalization is extremely important when comparing two samples of any data type. Consider a model that detects the keypoints of a hand from two separate images. Depending on the relative location of the hand in the image, the coordinates will vary greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = train_dataset[0]\n",
    "x2 = train_dataset[10000]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(-90, -90)\n",
    "plot_hands3d(ax, x1, 'b')\n",
    "plot_hands3d(ax, x2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization can resolve the change in relative size so that a fair comparison can be made. For this particular dataset, we will normalize each sample based on its size and set the origin to be the center of the palm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hand(points):\n",
    "    \"\"\"Normalize the hand and center it on the palm\"\"\"\n",
    "\n",
    "    center = points[-1]  # The palm keypoint\n",
    "    scale_factor = 1.0 / (points.max(0)[0] - points.min(0)[0])\n",
    "    norm_points = points.copy()\n",
    "    norm_points -= center\n",
    "    norm_points *= scale_factor\n",
    "\n",
    "    return norm_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_norm = normalize_hand(x1)\n",
    "x2_norm = normalize_hand(x2)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "ax1.view_init(-90, -90)\n",
    "plot_hands3d(ax1, x1_norm, 'b')\n",
    "plot_hands3d(ax1, x2_norm, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Closest Match\n",
    "\n",
    "In this notebook, your task is to complete the function `find_closest_match` to retrieve samples in the dataset that most closely match an input sample based on either L1 or L2 loss.\n",
    "\n",
    "**Requirements:**\n",
    "1. Define two functions to compute L1 and L2 loss.\n",
    "2. Normalize the input sample and each sample in the dataset.\n",
    "3. Compute the loss between the input sample and each sample in the dataset.\n",
    "4. Return the index of the sample in the dataset that has the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_match(x, dataset, loss_fn):\n",
    "    \"\"\"Searches the `dataset` for the sample that has the lowest\n",
    "    error as compared with `loss_fn`.\n",
    "    \"\"\"\n",
    "  x_norm = normalize_hand(x)\n",
    "    \n",
    "    min_loss = float('inf')\n",
    "    closest_sample = None\n",
    "    \n",
    "    for sample in dataset:\n",
    "        sample_norm = normalize_hand(sample)\n",
    "        loss = loss_fn(x_norm, sample_norm)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            closest_sample = sample\n",
    "    \n",
    "    return closest_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Your Implementation\n",
    "\n",
    "Be sure to fill in the definitions for `l1_loss`, `l2_loss`, and `find_closest_match`. You can then run the rest of the notebook to visualize the results of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(x, y):\n",
    "    \"\"\"Compute the L1 loss between two samples.\"\"\"\n",
    "    return 0\n",
    "\n",
    "def l2_loss(x, y):\n",
    "    \"\"\"Compute the L2 loss between two samples.\"\"\"\n",
    "    return 0\n",
    "\n",
    "# Search for the closest match\n",
    "input_idx = random.randint(0, len(test_dataset) - 1)\n",
    "input_sample = test_dataset[input_idx]\n",
    "l1_match = find_closest_match(input_sample, train_dataset, l1_loss)\n",
    "\n",
    "# Compute using L2 loss\n",
    "l2_match = find_closest_match(input_sample, train_dataset, l2_loss)\n",
    "\n",
    "# Normalize samples\n",
    "input_sample = normalize_hand(input_sample)\n",
    "l1_match = normalize_hand(l1_match)\n",
    "l2_match = normalize_hand(l2_match)\n",
    "\n",
    "# Visualize Results\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.set_title('L1 Match (Distance: {:.2f})'.format(l1_loss(input_sample, l1_match)))\n",
    "ax1.legend(['Input', 'Match'])\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.set_title('L2 Match (Distance: {:.2f})'.format(l2_loss(input_sample, l2_match)))\n",
    "plot_hands3d(ax1, input_sample, 'b')\n",
    "plot_hands3d(ax1, l1_match, 'r')\n",
    "plot_hands3d(ax2, input_sample, 'b')\n",
    "plot_hands3d(ax2, l2_match, 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse4310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
